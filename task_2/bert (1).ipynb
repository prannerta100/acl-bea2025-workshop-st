{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659bd06c-d708-42ee-9ad9-38ebbe39887e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d634f9d-ff6e-4c5d-8850-7306fcca1305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('train.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba6979a-384a-4679-83db-51288b1b814a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"Great, you've correctly identified the cost of the meat, now let's focus on calculating the total cost of meat for all the sandwiches needed.\",\n",
       " 'annotation': {'Mistake_Identification': 'Yes',\n",
       "  'Mistake_Location': 'Yes',\n",
       "  'Providing_Guidance': 'Yes',\n",
       "  'Actionability': 'Yes'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['tutor_responses']['Sonnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7bae0b3-5043-4b1a-9dab-16bff1df17c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train = []\n",
    "for i in range(0,len(data)):\n",
    "    for j in data[i]['tutor_responses']:\n",
    "        data_train.append({\"conversation_history\": data[i]['conversation_history'],\n",
    "        \"response\": data[i]['tutor_responses'][j]['response'],\n",
    "         \"annotation\":{\"Mistake_Location\":data[i]['tutor_responses'][j]['annotation']['Mistake_Location']}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca33992-5206-4e2f-9eb0-e109bd996889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample data (replace with your full dataset)\n",
    "\"\"\"data = [\n",
    "    {\n",
    "        \"conversation_history\": \"Tutor: Please explain... Student: ...\",\n",
    "        \"annotation\": {\"Mistake_Location\": \"Yes\"}\n",
    "    },\n",
    "    {\n",
    "        \"conversation_history\": \"Tutor: Can you try again... Student: ...\",\n",
    "        \"annotation\": {\"Mistake_Location\": \"No\"}\n",
    "    },\n",
    "    # Add more examples...\n",
    "]\n",
    "\"\"\"\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_train)\n",
    "label_map = {\n",
    "    \"No\": 0,\n",
    "    \"Yes\": 1,\n",
    "    \"To some extent\": 2  # or whatever the third label is\n",
    "}\n",
    "\n",
    "df[\"label\"] = df[\"annotation\"].apply(lambda x: label_map.get(x[\"Mistake_Location\"], -1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f439dd98-9fb7-4054-bbff-f0ead4faa5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>response</th>\n",
       "      <th>annotation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Now that we know the cost of 1 pound of meat i...</td>\n",
       "      <td>{'Mistake_Location': 'To some extent'}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>You're close, but I notice that you calculated...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>That's correct. So, if 1 pound of meat costs $...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>It seems like you've calculated the cost as if...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>Tutor: We can compare the ratios more easily i...</td>\n",
       "      <td>That's close, but the correct decimal value of...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>Tutor: We can compare the ratios more easily i...</td>\n",
       "      <td>Tutor: I see where you're going, but when we c...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>Tutor: We can compare the ratios more easily i...</td>\n",
       "      <td>I appreciate your attempt, but let's work toge...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>Tutor: We can compare the ratios more easily i...</td>\n",
       "      <td>Let's try that again. To convert 2/3 to a deci...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>Tutor: We can compare the ratios more easily i...</td>\n",
       "      <td>Remember, a decimal represents a part of a who...</td>\n",
       "      <td>{'Mistake_Location': 'Yes'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2067 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   conversation_history  \\\n",
       "0     Tutor: Hi, could you please provide a step-by-...   \n",
       "1     Tutor: Hi, could you please provide a step-by-...   \n",
       "2     Tutor: Hi, could you please provide a step-by-...   \n",
       "3     Tutor: Hi, could you please provide a step-by-...   \n",
       "4     Tutor: Hi, could you please provide a step-by-...   \n",
       "...                                                 ...   \n",
       "2062  Tutor: We can compare the ratios more easily i...   \n",
       "2063  Tutor: We can compare the ratios more easily i...   \n",
       "2064  Tutor: We can compare the ratios more easily i...   \n",
       "2065  Tutor: We can compare the ratios more easily i...   \n",
       "2066  Tutor: We can compare the ratios more easily i...   \n",
       "\n",
       "                                               response  \\\n",
       "0     Great, you've correctly identified the cost of...   \n",
       "1     Now that we know the cost of 1 pound of meat i...   \n",
       "2     You're close, but I notice that you calculated...   \n",
       "3     That's correct. So, if 1 pound of meat costs $...   \n",
       "4     It seems like you've calculated the cost as if...   \n",
       "...                                                 ...   \n",
       "2062  That's close, but the correct decimal value of...   \n",
       "2063  Tutor: I see where you're going, but when we c...   \n",
       "2064  I appreciate your attempt, but let's work toge...   \n",
       "2065  Let's try that again. To convert 2/3 to a deci...   \n",
       "2066  Remember, a decimal represents a part of a who...   \n",
       "\n",
       "                                  annotation  label  \n",
       "0                {'Mistake_Location': 'Yes'}      1  \n",
       "1     {'Mistake_Location': 'To some extent'}      2  \n",
       "2                {'Mistake_Location': 'Yes'}      1  \n",
       "3                {'Mistake_Location': 'Yes'}      1  \n",
       "4                {'Mistake_Location': 'Yes'}      1  \n",
       "...                                      ...    ...  \n",
       "2062             {'Mistake_Location': 'Yes'}      1  \n",
       "2063             {'Mistake_Location': 'Yes'}      1  \n",
       "2064             {'Mistake_Location': 'Yes'}      1  \n",
       "2065             {'Mistake_Location': 'Yes'}      1  \n",
       "2066             {'Mistake_Location': 'Yes'}      1  \n",
       "\n",
       "[2067 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61322aa1-a5f9-4af6-96f6-7f27fd293c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine the input columns\n",
    "def combine_inputs(row):\n",
    "    return f\"CONVERSATION: {row['conversation_history']} RESPONSE: {row['response']}\"\n",
    "\n",
    "df[\"text\"] = df.apply(combine_inputs, axis=1)\n",
    "\n",
    "# Train/test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfb80816-bf57-4065-b25e-2e0cb83ec552",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CONVERSATION: Tutor: Hi, could you please provide a step-by-step solution for the question below? The question is: Every month, Madeline has to buy food, treats, and medicine for her dog. Food costs $25 per week. Treats cost $20 per month. Medicine costs $100 per month. How much money does Madeline spend on her dog per year if there are 4 weeks in a month?\\xa0\\n\\xa0Student: In a month, Madeline spends 4 x $25 = $100 on food for her dog.\\nIn a year, Madeline spends 12 x $100 = $1200 on medicine for her dog.\\nIn a year, Madeline spends 12 x $20 = $240 on treats for her dog.\\nTherefore, Madeline spends a total of $100 + $1200 + $240 = $1540 on her dog per year.\\n\\xa01540\\xa0\\n\\xa0Tutor: Stephanie, please look at your workings out for the food amount. is your answer relevant to cost per year or month?\\xa0\\n\\xa0Student: Oh, I see. My answer is relevant to cost per month, not per year. So, I should multiply the cost per month by 12 to get the cost per year.\\xa0\\n\\xa0Tutor: brilliant, what would that new total be?\\xa0\\xa0\\n\\xa0Student: The new total would be 12 x ($25 + $20 + $100) = $1540 per year.\\xa0\\n\\xa0Tutor: you would need to work out the cost of the food per year first before you add medicine and treats\\xa0\\xa0\\n\\xa0Student: Yes, I understand now. The cost of food per year would be 4 x $25 = $100. So, the total cost of food, medicine, and treats per year would be $100 + $1200 + $240 = $1540. RESPONSE: You're on the right track with the food cost per year, but remember to multiply the weekly cost by the number of weeks in a year, not just the number of months.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df2f67e8-b0a0-4aff-90d4-e5f85d3953c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c89e62d149469eb88318a411f3855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3036ba5fa5b04ac181b26a2a94e9b384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b04c9d1037a424483c3f1143f4403ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec050cff88034890a0b241ddaa3c102a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d562e439dab0434e82c095c353ef0880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "roberta-base does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m MistakeDataset(val_encodings, val_labels)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load model for 3-class classification\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroberta-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(pred):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4251\u001b[0m     gguf_file\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4254\u001b[0m ):\n\u001b[1;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4258\u001b[0m     )\n\u001b[0;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:1080\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m   1072\u001b[0m has_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision,\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxies\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxies,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[1;32m   1078\u001b[0m }\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1081\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for TensorFlow weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1087\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1088\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for Flax weights. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: roberta-base does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Dataset class\n",
    "class MistakeDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = MistakeDataset(train_encodings, train_labels)\n",
    "val_dataset = MistakeDataset(val_encodings, val_labels)\n",
    "\n",
    "# Load model for 3-class classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3, from_tf=True)\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f52283aa-8dfc-43d7-a00c-bcc0e1405707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "bert-base-uncased does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m MistakeDataset(val_encodings, val_labels)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Load the model for 3-class classification\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Check if multiple GPUs are available\u001b[39;00m\n\u001b[1;32m     41\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4251\u001b[0m     gguf_file\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4254\u001b[0m ):\n\u001b[1;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4258\u001b[0m     )\n\u001b[0;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:1080\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m   1072\u001b[0m has_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision,\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxies\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxies,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[1;32m   1078\u001b[0m }\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1081\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for TensorFlow weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1087\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1088\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for Flax weights. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: bert-base-uncased does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine the conversation and response columns\n",
    "def combine_inputs(row):\n",
    "    return f\"CONVERSATION: {row['conversation_history']} RESPONSE: {row['response']}\"\n",
    "\n",
    "df[\"text\"] = df.apply(combine_inputs, axis=1)\n",
    "\n",
    "# Train/test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Dataset class\n",
    "class MistakeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = MistakeDataset(train_encodings, train_labels)\n",
    "val_dataset = MistakeDataset(val_encodings, val_labels)\n",
    "\n",
    "# Load the model for 3-class classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Training on {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    per_device_train_batch_size=2,  # Decrease batch size for memory\n",
    "    per_device_eval_batch_size=2,   # Decrease batch size for memory\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",  # Log directory\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # Save model at each epoch\n",
    "    fp16=True,  # Use mixed precision to save memory and speed up training\n",
    "    no_cuda=False,  # Make sure it runs on GPU\n",
    "    dataloader_num_workers=4,  # Increase for faster data loading if using a multi-core CPU\n",
    "    report_to=\"none\",  # Disable reporting to platforms like wandb if not needed\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./mistake_detection_model\")\n",
    "tokenizer.save_pretrained(\"./mistake_detection_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494da49-ff24-4857-99c0-652d236464b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
